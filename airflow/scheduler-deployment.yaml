# Source: airflow/templates/scheduler/scheduler-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-scheduler
  labels:
    app: airflow
    component: scheduler
    chart: airflow-7.13.3
    release: airflow
    heritage: Helm
spec:
  replicas: 1
  strategy:
    # this is safe as long as `maxSurge` is 0
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 100%
  selector:
    matchLabels:
      app: airflow
      component: scheduler
      release: airflow
  template:
    metadata:
      annotations:
        checksum/config-env: 7ef725357dcc3250bde412d19d3afe6466d279da663c28aa5c414d5c7f91909c
        checksum/config-git-clone: 8d91f664c6efe85acde0ff82ffa86fb277dec3b0b52fd6ca36cc13aea855400a
        checksum/config-scripts: 05cf92e4968d8817e918334edf85fdbc3e91838c0ab2f14bb54f384ef120f97a
        checksum/config-variables-pools: 533fb34fd0b107f562e353341d3923e4d1a19216554ada1bac2be84432df454f
        checksum/secret-connections: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: scheduler
        release: airflow
    spec:
      restartPolicy: Always
      serviceAccountName: airflow
      containers:
        - name: airflow-scheduler
          image: apache/airflow:1.10.12-python3.6
          imagePullPolicy: IfNotPresent
          envFrom:
            - configMapRef:
                name: "airflow-env"
          env:            
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
          resources:
            {}
          volumeMounts:
            - name: scripts
              mountPath: /home/airflow/scripts
            - name: variables-pools
              mountPath: /home/airflow/variables-pools/
          command:
            - "/usr/bin/dumb-init"
            - "--"
          args:
            - "bash"
            - "-c"
            - >
              true \
               && mkdir -p /home/airflow/.local/bin \
               && export PATH="/home/airflow/.local/bin:$PATH" \
               && echo "*** executing Airflow initdb..." \
               && airflow initdb \
               && echo "*** adding Airflow variables..." \
               && airflow variables -i /home/airflow/variables-pools/variables.json \
               && echo "*** adding Airflow pools..." \
               && airflow pool -i /home/airflow/variables-pools/pools.json \
               && echo "*** running scheduler..." \
               && exec airflow scheduler -n -1
          livenessProbe:
            initialDelaySeconds: 300
            periodSeconds: 30
            failureThreshold: 5
            exec:
              command:
              - python
              - -Wignore
              - -c
              - |
                import os
                os.environ['AIRFLOW__CORE__LOGGING_LEVEL'] = 'ERROR'
                os.environ['AIRFLOW__LOGGING__LOGGING_LEVEL'] = 'ERROR'
                from airflow.jobs.scheduler_job import SchedulerJob
                from airflow.utils.net import get_hostname
                import sys
                job = SchedulerJob.most_recent_job()
                sys.exit(0 if job.is_alive() and job.hostname == get_hostname() else 1)
      volumes:
        - name: scripts
          configMap:
            name: airflow-scripts
            defaultMode: 0755
        - name: variables-pools
          configMap:
            name: airflow-variables-pools
            defaultMode: 0755
---
